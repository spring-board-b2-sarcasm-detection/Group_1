{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c376c6d-f06b-45a1-82fb-6f9082ff1d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import sentencepiece as spm\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b969401-6b68-49a3-9b47-98a891ef520f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>Trumpbart</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>nba</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>Creepeth</td>\n",
       "      <td>nfl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>icebrotha</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>cush2push</td>\n",
       "      <td>MaddenUltimateTeam</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-30 17:00:13</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment     author   \n",
       "0      0                                         NC and NH.  Trumpbart  \\\n",
       "1      0  You do know west teams play against west teams...  Shbshb906   \n",
       "2      0  They were underdogs earlier today, but since G...   Creepeth   \n",
       "3      0  This meme isn't funny none of the \"new york ni...  icebrotha   \n",
       "4      0                    I could use one of those tools.  cush2push   \n",
       "\n",
       "            subreddit  score  ups  downs     date          created_utc   \n",
       "0            politics      2   -1     -1  2016-10  2016-10-16 23:55:23  \\\n",
       "1                 nba     -4   -1     -1  2016-11  2016-11-01 00:24:10   \n",
       "2                 nfl      3    3      0  2016-09  2016-09-22 21:45:37   \n",
       "3  BlackPeopleTwitter     -8   -1     -1  2016-10  2016-10-18 21:03:47   \n",
       "4  MaddenUltimateTeam      6   -1     -1  2016-12  2016-12-30 17:00:13   \n",
       "\n",
       "                                      parent_comment  \n",
       "0  Yeah, I get that argument. At this point, I'd ...  \n",
       "1  The blazers and Mavericks (The wests 5 and 6 s...  \n",
       "2                            They're favored to win.  \n",
       "3                         deadass don't kill my buzz  \n",
       "4  Yep can confirm I saw the tool they use for th...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "file_path = '/Users/jashanjeetsingh/Downloads/train-balanced-sarcasm.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ecff862-3fbd-45e6-893b-af603511e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text for tokenization\n",
    "text_sample = df['comment'].iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8717c197-d0cc-425e-bc3f-ac0253181eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace Tokens: ['NC', 'and', 'NH.']\n"
     ]
    }
   ],
   "source": [
    "# Whitespace Tokenization\n",
    "whitespace_tokens = text_sample.split()\n",
    "print(\"Whitespace Tokens:\", whitespace_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34afb630-5379-4b7a-a3e9-6fb1ae73a115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation Tokens: ['NC', 'and', 'NH', '.']\n"
     ]
    }
   ],
   "source": [
    "# Punctuation-based Tokenization\n",
    "punctuation_tokens = re.findall(r'\\w+|[^\\w\\s]', text_sample, re.UNICODE)\n",
    "print(\"Punctuation Tokens:\", punctuation_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b76542f-691e-462f-8193-e7081ed83bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Word Tokens: ['NC', 'and', 'NH', '.']\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenization using spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_tokens = [token.text for token in nlp(text_sample)]\n",
    "print(\"spaCy Word Tokens:\", spacy_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b8bc1b1-338d-4d93-b76e-17cd3a2c19fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a BPE model\n",
    "with open('comments.txt', 'w') as f:\n",
    "    for comment in df['comment']:\n",
    "        f.write(f\"{comment}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd0814de-dd4d-4611-9d20-08e9e7b8f89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Subword Tokens: ['▁N', 'C', '▁and', '▁N', 'H', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=comments.txt --model_prefix=m --vocab_size=5000\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: comments.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 5000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: comments.txt\n",
      "trainer_interface.cc(380) LOG(WARNING) Found too long line (10000 > 4192).\n",
      "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(124) LOG(WARNING) Too many sentences are loaded! (1010801), which may slow down training.\n",
      "trainer_interface.cc(126) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(129) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 1010801 sentences\n",
      "trainer_interface.cc(416) LOG(INFO) Skipped 25 too long sentences.\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=58107090\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9541% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=81\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999541\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 1010801 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=27952749\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 477800 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 1010801\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 496927\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 496927 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=180742 obj=11.2964 num_tokens=1120434 num_tokens/piece=6.19908\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=161793 obj=9.07585 num_tokens=1133264 num_tokens/piece=7.00441\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=121316 obj=9.04288 num_tokens=1177356 num_tokens/piece=9.70487\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=121206 obj=9.03191 num_tokens=1180187 num_tokens/piece=9.73703\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=90903 obj=9.0721 num_tokens=1237726 num_tokens/piece=13.6159\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=90900 obj=9.06363 num_tokens=1237754 num_tokens/piece=13.6167\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=68175 obj=9.1181 num_tokens=1300373 num_tokens/piece=19.074\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=68175 obj=9.10716 num_tokens=1300313 num_tokens/piece=19.0732\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=51131 obj=9.17721 num_tokens=1367412 num_tokens/piece=26.7433\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=51131 obj=9.16407 num_tokens=1367255 num_tokens/piece=26.7402\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=38348 obj=9.25405 num_tokens=1438013 num_tokens/piece=37.499\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=38348 obj=9.23757 num_tokens=1437925 num_tokens/piece=37.4967\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=28761 obj=9.34362 num_tokens=1510810 num_tokens/piece=52.5298\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=28761 obj=9.32332 num_tokens=1510664 num_tokens/piece=52.5247\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=21570 obj=9.45454 num_tokens=1586750 num_tokens/piece=73.5628\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=21570 obj=9.42962 num_tokens=1586733 num_tokens/piece=73.562\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=16177 obj=9.58672 num_tokens=1666962 num_tokens/piece=103.045\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=16177 obj=9.55581 num_tokens=1666804 num_tokens/piece=103.035\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12132 obj=9.73621 num_tokens=1750016 num_tokens/piece=144.248\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=12132 obj=9.69974 num_tokens=1749988 num_tokens/piece=144.246\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=9099 obj=9.91001 num_tokens=1836906 num_tokens/piece=201.88\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9099 obj=9.86799 num_tokens=1836993 num_tokens/piece=201.89\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6824 obj=10.1068 num_tokens=1929300 num_tokens/piece=282.723\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6824 obj=10.0567 num_tokens=1929162 num_tokens/piece=282.703\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5500 obj=10.2514 num_tokens=1996570 num_tokens/piece=363.013\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5500 obj=10.2131 num_tokens=1996589 num_tokens/piece=363.016\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    }
   ],
   "source": [
    "# Subword Tokenization using SentencePiece (BPE)\n",
    "spm.SentencePieceTrainer.Train('--input=comments.txt --model_prefix=m --vocab_size=5000')\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "bpe_tokens = sp.encode_as_pieces(text_sample)\n",
    "print(\"BPE Subword Tokens:\", bpe_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "086ccd76-7ada-40d9-b2a4-e8c5fea1afb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aef96e2d7ce4a46ae2938ce5f9989dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b91330c3521467fa91fdb1ddd59b8c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851c53542eb34588b1abb91e5cbc95c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40cc35f8f4c46988d708a7f9995ae97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2f5854b2384e15a8ae99baa0779da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4750fce21ed24e099992d5f5144e5759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers Word Tokens: ['nc', 'and', 'nh', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "file_path = '/Users/jashanjeetsingh/Downloads/train-balanced-sarcasm.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "text_sample = df['comment'].iloc[0]\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", force_download=True)\n",
    "transformers_tokens = tokenizer.tokenize(text_sample)\n",
    "print(\"Transformers Word Tokens:\", transformers_tokens)\n",
    "\n",
    "#We are facing this error because Jupyter is unable to display the widget, whereas it runs fine on a normal python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c4f6ae1-9da6-4bbb-bdc6-ac4e79b10c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Tokens: ['N', 'C', ' ', 'a', 'n', 'd', ' ', 'N', 'H', '.']\n"
     ]
    }
   ],
   "source": [
    "# Character Tokenization\n",
    "character_tokens = list(text_sample)\n",
    "print(\"Character Tokens:\", character_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6cc73c3-6375-4de0-a815-f38aaed54d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jashan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
