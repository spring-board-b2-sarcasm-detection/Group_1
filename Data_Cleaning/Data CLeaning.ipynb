{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a02aaa7-6211-4baf-bd2a-daa34dfe5e1e",
   "metadata": {},
   "source": [
    "# DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c138890-b07c-470a-8a20-b4d964f983f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\guded\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \n",
    "            \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \n",
    "            \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \n",
    "            \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n",
    "            \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\",\n",
    "            \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \n",
    "            \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \n",
    "            \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \n",
    "            \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\",\n",
    "            \"more\", \"most\", \"other\", \"some\", \"such\" \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \n",
    "            \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "\n",
    "def remove_stopwords_vectorized(text_list):\n",
    "  text_list = np.char.lower(text_list) \n",
    "  words = np.char.split(text_list)\n",
    "  mask = ~np.isin(words, stop_words)\n",
    "  return pd.Series([' '.join(w) for w in words[mask]])\n",
    "df['cleaned_text'] = df['comment'].apply(remove_stopwords_vectorized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaf9120-912c-45e0-b229-f9a82b34d8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3b7f0d5f-a131-4d85-b19a-ad9c9fa1898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    \n",
    "    \n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    \n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "843daa09-382b-4140-8c49-07b1d5c47ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.12.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7.0 in c:\\users\\guded\\anaconda3\\lib\\site-packages (from emoji) (4.9.0)\n",
      "Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
      "   ---------------------------------------- 0.0/431.4 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/431.4 kB ? eta -:--:--\n",
      "   ----- --------------------------------- 61.4/431.4 kB 656.4 kB/s eta 0:00:01\n",
      "   ------------- -------------------------- 143.4/431.4 kB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 348.2/431.4 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  430.1/431.4 kB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 431.4/431.4 kB 2.1 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.12.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "23c7a9f4-31e4-4de4-940d-b0ed53988aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\guded\\anaconda3\\lib\\site-packages (2.12.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7.0 in c:\\users\\guded\\anaconda3\\lib\\site-packages (from emoji) (4.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5d61018-f9e1-4540-9c2f-812771600c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "import re  # Import the re module\n",
    "\n",
    "def remove_html(text):\n",
    "  \"\"\"Removes HTML tags from text.\"\"\"\n",
    "  return re.sub(r'<.*?>', \"\", text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "  \"\"\"Removes punctuation characters from text.\"\"\"\n",
    "  punctuation = \"\"\"!\"#$%&()*+, -./:;<=>?@[\\]^_`{|}~\"\"\"  \n",
    "  for char in punctuation:\n",
    "    text = text.replace(char, \" \")\n",
    "  return text\n",
    "\n",
    "def remove_numbers(text):\n",
    "  \"\"\"Removes digits (numbers) from text.\"\"\"\n",
    "  return re.sub(\"\\d+\", \"\", text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "  \"\"\"Removes emojis from text using the emoji library.\"\"\"\n",
    "  return emoji.demojize(text)\n",
    "\n",
    "# Example usage\n",
    "text = \"This text has <html tags>, punctuation?! and emojis . I haveremove all of them.\"\n",
    "\n",
    "cleaned_text = remove_html(text)\n",
    "cleaned_text = remove_punctuation(cleaned_text)\n",
    "cleaned_text = remove_numbers(cleaned_text)\n",
    "cleaned_text = remove_emoji(cleaned_text)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0d80501d-8060-4f62-99cf-2989f26fd6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_length():\n",
    "    list2=[]\n",
    "    for sentence in text:\n",
    "        count=len(sentence)\n",
    "        list2.append(count)\n",
    "    avg_word_length=sum(list2)/len(text)\n",
    "    \n",
    "    return avg_word_length\n",
    "def average_senetnce_length(text):\n",
    "    sum1=0\n",
    "    for sentence in text:\n",
    "        count=len(sentence.split())\n",
    "        sum1=sum1+count\n",
    "        \n",
    "    return (sum1/len(text))\n",
    "def count_exclamation(text):\n",
    "    sum1=0\n",
    "    for i in text:\n",
    "        if '!' in i:\n",
    "            sum1=sum1+1\n",
    "    return sum1\n",
    "\n",
    "def count_question(text):\n",
    "    sum1=0\n",
    "    for i in text:\n",
    "        if '?' in i:\n",
    "            sum1=sum1+1\n",
    "    return sum1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e6f89a42-f53e-440a-859f-b71022738dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (808351, 6)\n",
      "X_test shape: (202088, 6)\n",
      "y_train shape: (808351,)\n",
      "y_test shape: (202088,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop(columns=['label'])  \n",
    "y = df['ups']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6ebb92de-3cbb-49d3-8828-2d2d2809c738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "      <td>[[nc, and, nh.]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "      <td>[[you, do, know, west, teams, play, against, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>They're favored to win.</td>\n",
       "      <td>[[they, were, underdogs, earlier, today,, but,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "      <td>[[this, meme, isn't, funny, none, of, the, \"ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "      <td>[[i, could, use, one, of, those, tools.]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  score  ups  \\\n",
       "0      0                                         NC and NH.      2   -1   \n",
       "1      0  You do know west teams play against west teams...     -4   -1   \n",
       "2      0  They were underdogs earlier today, but since G...      3    3   \n",
       "3      0  This meme isn't funny none of the \"new york ni...     -8   -1   \n",
       "4      0                    I could use one of those tools.      6   -1   \n",
       "\n",
       "   downs                                     parent_comment  \\\n",
       "0     -1  Yeah, I get that argument. At this point, I'd ...   \n",
       "1     -1  The blazers and Mavericks (The wests 5 and 6 s...   \n",
       "2      0                            They're favored to win.   \n",
       "3     -1                         deadass don't kill my buzz   \n",
       "4     -1  Yep can confirm I saw the tool they use for th...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0                                   [[nc, and, nh.]]  \n",
       "1  [[you, do, know, west, teams, play, against, w...  \n",
       "2  [[they, were, underdogs, earlier, today,, but,...  \n",
       "3  [[this, meme, isn't, funny, none, of, the, \"ne...  \n",
       "4          [[i, could, use, one, of, those, tools.]]  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc72254-9354-4c8b-a942-ba59e6798f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
